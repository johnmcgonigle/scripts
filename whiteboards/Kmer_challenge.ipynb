{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmer challenge:\n",
    "\n",
    "\n",
    "## Problem 1:\n",
    "`There are 4096 (4^6) possible DNA hexamers (right?).\n",
    "    ex. - AAAAAA, AAAAAT, AAAAAC, AAAAAG, AAAATA, ...`\n",
    "`Given a list of 400 hexamers, find a new hexamer that is the most possible 'different' hexamer from that list of hexamers.`\n",
    "\n",
    "The issue here is deciding on a metric by which to determine which of the strings of 6 bases is most 'different'. \n",
    "One approach would be to compute a distance metric between all strings, such as the Levenshtein distance, however this is a pairwise combinatorial that increases is complexity with each addition to the list. It is also possible to apply a dimension reduction approach, clustering the data on the basis of the composition into groups and then measuring distance from each group, but it then becomes difficult to discern what point to start at to then measure distance. The approach I am going to instead choose to take is an attempt to measure rareness or frequency of subunits of the hexamer as a proxy for differentness. \n",
    "\n",
    "Each hexamer is made up of a number of smaller kmers and so by taking the frequency of the constituent kmers we can look for rare motifs not shared by the majority of sequences. We can then further reduce the kmer we're using to compare to isolate sequences that are rarer and rarer. \n",
    "\n",
    "Take for example the hexamer ATCGAT:\n",
    "This is composed of two trimers ATC and GAT, but if we take a sliding window approach, this is actually composed of four trimers ATC, TCG, CGA, GAT. Storing these motifs and then differentiating between hexamer sequences on the basis of the rarity of the motif, is one way to do a rather blunt analysis across all sequences in a single iteration. I'm not claimning it's the best way, but it will at least rank the sequences in how common the constituent kmers that make up the hexamer are.\n",
    "\n",
    "### Code 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.0017080307006835938\n",
      "1\n",
      "0.0059299468994140625\n",
      "1\n",
      "0.03848695755004883\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from random import choice\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def make_kmer_list(k, n):\n",
    "    \"\"\" Returns a list of n length with DNA sequences of k bases.\"\"\"\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    rtn_lst = []\n",
    "    for i in range(n):\n",
    "        rtn_lst.append(''.join([choice(bases) for i in range(k)]))\n",
    "    return rtn_lst\n",
    "\n",
    "\n",
    "def sliding_window(k, seq):\n",
    "    \"\"\"\n",
    "    Returns all kmers of size k from a sequence seq.\n",
    "    \n",
    "    This gives us all the constituent kmers of size k that make up the sequences \n",
    "    \"\"\"\n",
    "    iterable = iter(seq)\n",
    "    result = tuple(islice(iterable, k))\n",
    "    if len(result) == k:\n",
    "        yield result\n",
    "    for kmer in iterable:\n",
    "        result = result[1:] + (kmer,)\n",
    "        yield result\n",
    "\n",
    "\n",
    "def record_kmers(lst_of_seqs, k):\n",
    "    \"\"\" \n",
    "    This function iterates over each of the DNA sequences and  produces all possible kmers and then records the \n",
    "    frequency of these kmers. In a seperate dictionary for each kmer we record every sequence under the key \n",
    "    of its corresponding kmer, so that we can examine these sequences later. \n",
    "    By design each sequence should be stored under multiple kmers.\n",
    "    \"\"\"\n",
    "    codon_dct = defaultdict(list)\n",
    "    codon_fequencies = defaultdict(int)\n",
    "\n",
    "    for i in range(len(lst_of_seqs)):\n",
    "        seq = lst_of_seqs[i]\n",
    "\n",
    "        codons = sliding_window(k, seq)\n",
    "\n",
    "        for codon in codons:\n",
    "            codon = ''.join(codon)  # kmers come back as a list of characters so we'll put them back together\n",
    "            codon_dct[codon].append(seq)  # add seqeuence to a list beneath the kmer \n",
    "            codon_fequencies[codon] += 1  # Incredment kmer frquency for each time it is 'seen'\n",
    "\n",
    "    return codon_dct, codon_fequencies\n",
    "\n",
    "\n",
    "def sort_dict_by_values(d, reversed):\n",
    "    return sorted(d, key=d.get, reverse=reversed)\n",
    "\n",
    "\n",
    "def get_target_sequences(codon_dct, sorted_codons):\n",
    "    \"\"\"\n",
    "    Return the rarest sequences as discerned by kmer frequency.\n",
    "    \n",
    "    Any sequence that contains two rare kmers can be included in the targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now we need to get a subset of targets on which to perform further analysis so:\n",
    "    kmer_not_found = True  # set a logic gate\n",
    "    i = 0  # value to keep track of our position\n",
    "    current_pool = codon_dct[sorted_codons[1]]  # we start with sequences containing the rarest codons as these\n",
    "    # are the most likely to be the most 'different'\n",
    "\n",
    "    # This is where we'll store our potential targets:\n",
    "    targets = []\n",
    "\n",
    "    # Iterate through until we find a target that contains 2 of the rarest codons.\n",
    "    while kmer_not_found:\n",
    "        i += 1\n",
    "        current_sequences = codon_dct[sorted_codons[i]]\n",
    "        for seq in current_sequences:\n",
    "            if seq in current_pool:\n",
    "                # if we find a sequence that has both the rarest and another rare kmer\n",
    "                # By design this will not stop iterations because if there are ten equally rare sequences then we need\n",
    "                # to differentiate between these.\n",
    "                kmer_not_found = False\n",
    "                targets.append(seq)\n",
    "            else:\n",
    "                current_pool.extend(current_sequences)           \n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def reduce_by_rarity(lst_of_sequences, k):\n",
    "    \"\"\" \n",
    "    For a given list of sequences, and a given size of k, we first differentiate the 'rarist' sequences\n",
    "    on the basis of kmers of size k and then if there is more than one returning result we decrement k by 1 reducing \n",
    "    the kmer size to further differentiate between this pool of now smaller atrgets until we either run out of 'k' \n",
    "    e.g. k = 1 and we are differntiating on the basis of single characters or there is only one target.\n",
    "    \"\"\"\n",
    "    sequences = lst_of_sequences\n",
    "    for i in reversed(range(1, k+1)):\n",
    "        # Record kmers present in each sequence, using a sliding window technique\n",
    "        # To minimise iterations we record the sequences containing the kmers and the kmer frequencies separately\n",
    "        kmer_dct, kmer_frequencies = record_kmers(sequences, i)\n",
    "\n",
    "        # Sort the kmers by their frequency so we evaluate the rarest codons first\n",
    "        sorted_kmers = sort_dict_by_values(kmer_frequencies, False)\n",
    "        \n",
    "        # Use the least frequent kmers as a search pattern to select the most different sequences\n",
    "        targets = get_target_sequences(kmer_dct, sorted_kmers)\n",
    "        \n",
    "        # If the list of sequences is one return the target, otherwise set the targets as the sequences, lower the \n",
    "        # value of k and begin again\n",
    "        if len(targets) == 1:\n",
    "            return targets\n",
    "        else:\n",
    "            sequences = targets\n",
    "    return targets\n",
    "\n",
    "\n",
    "def time_function_runtime(func, *args):\n",
    "    \"\"\"\n",
    "    Decorator function to allow me to keep track of run time.\n",
    "    Note this is a only a rough estimate and I'd use the timeit library for proper timing optimisation, as a \n",
    "    single run value is essentially only one data point and may be biased in one direction or another.\n",
    "    \"\"\"\n",
    "    def wrapper(*args):\n",
    "        t = time()  # Get time at start\n",
    "        func(*args)  # Run program\n",
    "        t1 = time()  # Get time at completion\n",
    "        print(t1-t)  # Print different between start and finish (run time)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@time_function_runtime\n",
    "def run(no_of_values, kmer_size, starting_window_size):\n",
    "    \"\"\" A function allowing me to specify the size of the list, the size of the starting values e.g. in this challenge\n",
    "    hexamers (kmer_size = 6), and the size of the sliding window to begin.\"\"\"\n",
    "    six_mers = make_kmer_list(kmer_size, no_of_values)  # Generate the kmers to iterate over\n",
    "    six_mers = list(set(six_mers)) # Make sure we don't have duplicates\n",
    "    targets = reduce_by_rarity(six_mers, starting_window_size)  # start with a sliding window of 3\n",
    "    print(len(targets))\n",
    "    return targets\n",
    "\n",
    "\n",
    "# I chose a starting window size of 4 as it should optimise the time slightly more by getting rid of the \n",
    "# really common sequences earlier when we're iterating over less subunits.\n",
    "run(40, 6, 4)\n",
    "run(400, 6, 4)\n",
    "run(4000, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:\n",
    "`Suppose that we are concerned with DNA duplex hexamers, find also the most possible 'different' duplex hexamer?\n",
    "    duplex ex.- AAAAAA     TTTTTT\n",
    "                ||||||  =  ||||||\n",
    "                TTTTTT     AAAAAA`\n",
    "                \n",
    "One method to solve this would simply be to remove duplicated strings from the intial search space and keep only sequences where their complimentary sequence isn't present.\n",
    "\n",
    "This is obviously still assuming the same inputs e.g. AAAAAA and TTTTTT as opposed to the representation above, but this could likely be handled by finding a string way of expressing the above if requires e.g. AAAAAATTTTTT= although I don't see any benefit in this and it would probably influence the kmer search space.\n",
    "\n",
    "## Code 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.0008950233459472656\n",
      "1\n",
      "0.008295774459838867\n",
      "1\n",
      "0.1419692039489746\n"
     ]
    }
   ],
   "source": [
    "def create_compliment(sequence):\n",
    "    \"\"\" Function to convert between an input sequence and it's complimentary sequence.\"\"\"\n",
    "    base_mappings = {'A':'T','T':'A','C':'G','G':'C'}\n",
    "    return ''.join([base_mappings[base] for base in list(sequence)])\n",
    "\n",
    "def remove_complimentary_sequence(lst_of_sequences):\n",
    "    \"\"\"\n",
    "    Iterate over the list of sequences and only keep those where the sequence and its complimentary sequence are\n",
    "    not present.\n",
    "    \"\"\"\n",
    "    rtn_lst = []\n",
    "    for seq in lst_of_sequences:\n",
    "        if seq not in rtn_lst and create_compliment(seq) not in rtn_lst:\n",
    "            rtn_lst.append(seq)\n",
    "    return rtn_lst\n",
    "\n",
    "# Now modify run() to use the new code\n",
    "@time_function_runtime\n",
    "def run(no_of_values, kmer_size, starting_window_size):\n",
    "    \"\"\" A function allowing me to specify the size of the list, the size of the starting values e.g. in this challenge\n",
    "    hexamers (kmer_size = 6), and the size of the sliding window to begin.\"\"\"\n",
    "    six_mers = make_kmer_list(kmer_size, no_of_values)  # Generate the kmers to iterate over\n",
    "    six_mers = list(set(six_mers)) # Make sure we don't have duplicates\n",
    "    six_mers = remove_complimentary_sequence(six_mers)\n",
    "    targets = reduce_by_rarity(six_mers, starting_window_size)  # start with a sliding window of 3\n",
    "    print(len(targets))\n",
    "    return targets\n",
    "\n",
    "run(40, 6, 4)\n",
    "run(400, 6, 4)\n",
    "run(4000, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS:\n",
    "`Can your code work for longer k-mers? At what k-mer length does your code become a processing nightmare`\n",
    "The constant reduction of the search space by starting at a greater sliding window size, that is a larger kmer, reduces the issue of a growing sequence length, but by no means gets rid of it. Also currently it decrements by 1, which is massively inefficent at large kmer sizes e.g. imagine the difference in kmer subunits for a kmer of size 2000 with subunits of size 500 and 499. It's probably not going to be an enlightening difference.\n",
    "\n",
    "The examples below show the run time of the program for sequences of length 200 and the again for sequences of length 2000. \n",
    "\n",
    "Already it begins to slow, and an order of magnitude more would put the process into the minutes.\n",
    "\n",
    "It somewhat depends on where the process is running and how long is too long, but by length 200,000 bases per sequence this approach would almost certainly become extremely tedious and redudant.\n",
    "\n",
    "As alluded to above part of this is the issue of decrementing by 1 for each new target when the subunit size is larger than 20, i.e. we're looking at kmers of size 100 within the kmer of size 2000. Greater distance between the kmer subunits during the analysis probably inprove the speed of the process by decrementing by ks of greater size such as ten or twenty. This woukld reduce the number of interations, although this would take optimisation, to ensure we weren't throwing away information.\n",
    "\n",
    "In conclusion the current system would work OK upto kmers of 2000 for smallish lists e.g. 4000 or less taking 40 seconds to return a target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.030164003372192383\n",
      "1\n",
      "0.20389413833618164\n",
      "1\n",
      "2.5957210063934326\n",
      "1\n",
      "0.30382704734802246\n",
      "1\n",
      "3.416584014892578\n",
      "1\n",
      "70.41143441200256\n"
     ]
    }
   ],
   "source": [
    "run(40,200, 20)\n",
    "run(400, 200, 20)\n",
    "run(4000, 200, 20)\n",
    "\n",
    "run(40,2000, 100)\n",
    "run(400, 2000, 100)\n",
    "# run(4000, 2000, 100) - this takes 40 seconds so I've commented it out in case you run it.\n",
    "\n",
    "# run(4000, 2000, 500) - this takes 70 seconds, becase you're iterating 500 times. \n",
    "# This could be fixed by decrementing by 10 or 20 instead of 1 for kmers above 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION ON EXTENSION FEASIBILITY/STRATEGY:\n",
    "`Suppose we wanted to find the most possible 'different' 20-mer in an entire genome (so the list is now all 20-mers in a whole genome!) - perhaps that's not even realistically possible...\n",
    "What (if any) are some strategies that you might employ in order to find the most possible 'different' (or at least a very 'different') 20-mer?`\n",
    "\n",
    "This depends entirely on how your classifying difference, if you want to perform a distance calculation the pairwise combination of 20-mer in a 3B base pairs looks instantly unfeasible, as would be using the kmer anaylsis outlined above as it would take forever to finish. If you simply wanted to ask have I seen this kmer or part of this kmer before, then a bloom filter approach may lend itself to this task. However, this doesn't really tell you difference as much as distinctness.\n",
    "\n",
    "If you were confident you could homogonise the genome, that is mix up the location of these 20-mer reads sufficiently, you might be able to break the problem down in a map-reduce style manner performing an analysis on many sub-sections of the dataset in parallel. This does open itself up to the sub-population problem though, in that you are not longer comparing everything equally and sub-population structure might then bias the final result. \n",
    "\n",
    "Trying to extend my kmer approach, it is I suppose possible to perform this analysis if you chunked up the genome and mixed up the sequences in between each change in the sliding window size, essentially performing the rarity selection in microcosm many times. However, this risks population structure returning you a biased answer where you end up with a rare, but not the rarist sequence. \n",
    "\n",
    "Alternatively, if what you were interested in was difference from the norm, rather than difference from everything else then it may be possible to produce a representation of that norm that you can then compare against all the differnt 20-mers individually, allowing enormous parallelisation. At this point, it may then lend itself to string distance calculations as you would be reducing the number of pairwise combinations, although it seems unlikely that you would sufficently represent the norm with a small enough pool of sequences that this pairwise problem wouldn't still be extremely computationally intensive. \n",
    "\n",
    "Using this differentiation from the norm with my approach, assuming we could represent the 'norm' well enough, it would be feasible to characterise all kmers within this norm and then simply perform a kmer analysis for every new sequence looking if the kmers that compose the sequences are sufficiently different to be interesting. This may however simply return you kmers that result from sequencing or PCR error and thus are never seen in nature.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
